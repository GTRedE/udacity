{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Driving Car Engineer Nanodegree\n",
    "\n",
    "\n",
    "## Project 3: Behavioral Cloning\n",
    "August 2017\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "The goal of this project is to build a machine learning model that can successfully steer a car around a race track that it's never encountered before.\n",
    "\n",
    "The details for this project are located here at [Udacity's Github repo](https://github.com/udacity/CarND-Behavioral-Cloning-P3). My implementation of the project can be found [here at my Github repo](https://github.com/tommytracey/udacity/tree/master/self-driving-nano/projects/3-behavioral-cloning).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import cv2\n",
    "import keras\n",
    "import keras.backend\n",
    "from keras.callbacks import ModelCheckpoint, Callback\n",
    "from keras.layers.core import Activation, Dense, Dropout, Flatten, Lambda, Reshape\n",
    "from keras.layers.convolutional import Conv2D, Cropping2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify that Keras is using Tensforflow backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Keras backend uses Theano by default and changing it to Tensorflow can be tricky via Jupyter. Simply updating the config json file `$HOME/.keras/keras.json` as directed in [Keras backend documentation](https://keras.io/backend/) did not work for me. Trying to set it _before or after loading the notebook_ did not work eithter when using:\n",
    "\n",
    "`$ os.environ[\"KERAS_BACKEND\"]=\"tensorflow\"`\n",
    "\n",
    "The only way I could reliably set Tensorflow as the backend was to use the following command **UPON loading the notebook**:\n",
    "\n",
    "`$ KERAS_BACKEND=tensorflow jupyter notebook`\n",
    "\n",
    "(NOTE: You can also append `--NotebookApp.iopub_data_rate_limit=10000000000` to the above command if your notebook includes a lot of visualizations. This will help prevent the kernel from crashing and/or causing you to lose your connection to your AWS EC2 instance.)\n",
    "\n",
    "Re: the backend, **the cell below only provides a sanity check that the backend is configured as expected**. Note that the version of Tensorflow being used by Keras may be different than the one you typically run in your environment. \n",
    "\n",
    "[This post](https://www.nodalpoint.com/switch-keras-backend/) by Christos-Iraklis Tsatsoulis provides even more detail if you'd like to further understand the issues and automate the setup process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Keras version: ', keras.__version__)\n",
    "print('Tensorflow version: ', tf.__version__)\n",
    "print('Keras backend: ', keras.backend.backend())\n",
    "print('keras.backend.image_dim_ordering = ', keras.backend.image_dim_ordering())\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "if keras.backend.backend() != 'tensorflow':\n",
    "    raise BaseException(\"This script uses other backend\")\n",
    "else:\n",
    "    keras.backend.set_image_dim_ordering('tf')\n",
    "    print(\"\\nBackend OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Step 1: Load and preview the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Data Set (provided by Udacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total rows:  8036\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>center</th>\n",
       "      <th>left</th>\n",
       "      <th>right</th>\n",
       "      <th>steering</th>\n",
       "      <th>throttle</th>\n",
       "      <th>brake</th>\n",
       "      <th>speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IMG/center_2016_12_01_13_30_48_287.jpg</td>\n",
       "      <td>IMG/left_2016_12_01_13_30_48_287.jpg</td>\n",
       "      <td>IMG/right_2016_12_01_13_30_48_287.jpg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.148290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IMG/center_2016_12_01_13_30_48_404.jpg</td>\n",
       "      <td>IMG/left_2016_12_01_13_30_48_404.jpg</td>\n",
       "      <td>IMG/right_2016_12_01_13_30_48_404.jpg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.879630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IMG/center_2016_12_01_13_31_12_937.jpg</td>\n",
       "      <td>IMG/left_2016_12_01_13_31_12_937.jpg</td>\n",
       "      <td>IMG/right_2016_12_01_13_31_12_937.jpg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.453011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IMG/center_2016_12_01_13_31_13_037.jpg</td>\n",
       "      <td>IMG/left_2016_12_01_13_31_13_037.jpg</td>\n",
       "      <td>IMG/right_2016_12_01_13_31_13_037.jpg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.438419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IMG/center_2016_12_01_13_31_13_177.jpg</td>\n",
       "      <td>IMG/left_2016_12_01_13_31_13_177.jpg</td>\n",
       "      <td>IMG/right_2016_12_01_13_31_13_177.jpg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.418236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   center  \\\n",
       "0  IMG/center_2016_12_01_13_30_48_287.jpg   \n",
       "1  IMG/center_2016_12_01_13_30_48_404.jpg   \n",
       "2  IMG/center_2016_12_01_13_31_12_937.jpg   \n",
       "3  IMG/center_2016_12_01_13_31_13_037.jpg   \n",
       "4  IMG/center_2016_12_01_13_31_13_177.jpg   \n",
       "\n",
       "                                    left  \\\n",
       "0   IMG/left_2016_12_01_13_30_48_287.jpg   \n",
       "1   IMG/left_2016_12_01_13_30_48_404.jpg   \n",
       "2   IMG/left_2016_12_01_13_31_12_937.jpg   \n",
       "3   IMG/left_2016_12_01_13_31_13_037.jpg   \n",
       "4   IMG/left_2016_12_01_13_31_13_177.jpg   \n",
       "\n",
       "                                    right  steering  throttle  brake  \\\n",
       "0   IMG/right_2016_12_01_13_30_48_287.jpg       0.0       0.0    0.0   \n",
       "1   IMG/right_2016_12_01_13_30_48_404.jpg       0.0       0.0    0.0   \n",
       "2   IMG/right_2016_12_01_13_31_12_937.jpg       0.0       0.0    0.0   \n",
       "3   IMG/right_2016_12_01_13_31_13_037.jpg       0.0       0.0    0.0   \n",
       "4   IMG/right_2016_12_01_13_31_13_177.jpg       0.0       0.0    0.0   \n",
       "\n",
       "       speed  \n",
       "0  22.148290  \n",
       "1  21.879630  \n",
       "2   1.453011  \n",
       "3   1.438419  \n",
       "4   1.418236  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load UDACITY data into a list\n",
    "with open('data/udacity/driving_log.csv', newline='') as f:\n",
    "    udacity_data = list(csv.reader(f, skipinitialspace=True, delimiter=',', quoting=csv.QUOTE_NONE))\n",
    "    \n",
    "# Load UDACITY data from .csv and preview it in Pandas dataframe\n",
    "udacity_df = pd.read_csv('data/udacity/driving_log.csv', header=0)\n",
    "\n",
    "print('total rows: ', len(udacity_df))\n",
    "udacity_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Data Set (with addition of self-generated data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total rows:  18605\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>center</th>\n",
       "      <th>left</th>\n",
       "      <th>right</th>\n",
       "      <th>steering</th>\n",
       "      <th>throttle</th>\n",
       "      <th>brake</th>\n",
       "      <th>speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IMG/center_2016_12_01_13_30_48_287.jpg</td>\n",
       "      <td>IMG/left_2016_12_01_13_30_48_287.jpg</td>\n",
       "      <td>IMG/right_2016_12_01_13_30_48_287.jpg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.148290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IMG/center_2016_12_01_13_30_48_404.jpg</td>\n",
       "      <td>IMG/left_2016_12_01_13_30_48_404.jpg</td>\n",
       "      <td>IMG/right_2016_12_01_13_30_48_404.jpg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.879630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IMG/center_2016_12_01_13_31_12_937.jpg</td>\n",
       "      <td>IMG/left_2016_12_01_13_31_12_937.jpg</td>\n",
       "      <td>IMG/right_2016_12_01_13_31_12_937.jpg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.453011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IMG/center_2016_12_01_13_31_13_037.jpg</td>\n",
       "      <td>IMG/left_2016_12_01_13_31_13_037.jpg</td>\n",
       "      <td>IMG/right_2016_12_01_13_31_13_037.jpg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.438419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IMG/center_2016_12_01_13_31_13_177.jpg</td>\n",
       "      <td>IMG/left_2016_12_01_13_31_13_177.jpg</td>\n",
       "      <td>IMG/right_2016_12_01_13_31_13_177.jpg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.418236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   center  \\\n",
       "0  IMG/center_2016_12_01_13_30_48_287.jpg   \n",
       "1  IMG/center_2016_12_01_13_30_48_404.jpg   \n",
       "2  IMG/center_2016_12_01_13_31_12_937.jpg   \n",
       "3  IMG/center_2016_12_01_13_31_13_037.jpg   \n",
       "4  IMG/center_2016_12_01_13_31_13_177.jpg   \n",
       "\n",
       "                                    left  \\\n",
       "0   IMG/left_2016_12_01_13_30_48_287.jpg   \n",
       "1   IMG/left_2016_12_01_13_30_48_404.jpg   \n",
       "2   IMG/left_2016_12_01_13_31_12_937.jpg   \n",
       "3   IMG/left_2016_12_01_13_31_13_037.jpg   \n",
       "4   IMG/left_2016_12_01_13_31_13_177.jpg   \n",
       "\n",
       "                                    right  steering  throttle  brake  \\\n",
       "0   IMG/right_2016_12_01_13_30_48_287.jpg       0.0       0.0    0.0   \n",
       "1   IMG/right_2016_12_01_13_30_48_404.jpg       0.0       0.0    0.0   \n",
       "2   IMG/right_2016_12_01_13_31_12_937.jpg       0.0       0.0    0.0   \n",
       "3   IMG/right_2016_12_01_13_31_13_037.jpg       0.0       0.0    0.0   \n",
       "4   IMG/right_2016_12_01_13_31_13_177.jpg       0.0       0.0    0.0   \n",
       "\n",
       "       speed  \n",
       "0  22.148290  \n",
       "1  21.879630  \n",
       "2   1.453011  \n",
       "3   1.438419  \n",
       "4   1.418236  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data into list\n",
    "with open('data/track1/driving_log.csv', newline='') as f:\n",
    "    track1_data = list(csv.reader(f, skipinitialspace=True, delimiter=',', quoting=csv.QUOTE_NONE))\n",
    "\n",
    "# Load data from .csv and preview it in Pandas dataframe\n",
    "track1_df = pd.read_csv('data/track1/driving_log.csv', header=0)\n",
    "\n",
    "print('total rows: ', len(track1_df))\n",
    "track1_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Set: Initial Observations\n",
    "We can see from the table above that the driving data includes:\n",
    "- relative paths to .jpg images from three different camera angles (center, left, right)\n",
    "- floating point measurements of the vehicle's steering angle, throttle, brake, and speed\n",
    "- the data appears to be time series, although no time stamps are included"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview the driving images\n",
    "\n",
    "The driving images are the training features for our model. We need to look at samples of these images and start thinking about how various characteristics might affect the model (positively or negatively). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Preview a random set of images from each camera angle\n",
    "\n",
    "index = random.randint(0, len(udacity_df))\n",
    "img_dir = 'data/udacity/'\n",
    "\n",
    "center_img_orig = mpimg.imread(img_dir + udacity_data[index][0])\n",
    "left_img_orig = mpimg.imread(img_dir + udacity_data[index][1])\n",
    "right_img_orig = mpimg.imread(img_dir + udacity_data[index][2])\n",
    "\n",
    "center_steer = udacity_data[index][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Display visualizations in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "\n",
    "plt.subplot2grid((1, 3), (0, 0));\n",
    "plt.axis('off')\n",
    "plt.title('left camera')\n",
    "plt.text(0, left_img_orig.shape[0]+15, ('shape: ' + str(left_img_orig.shape)))\n",
    "plt.imshow(left_img_orig, cmap=\"gray\")\n",
    "\n",
    "plt.subplot2grid((1, 3), (0, 1));\n",
    "plt.axis('off')\n",
    "plt.title('center camera')\n",
    "plt.text(0, center_img_orig.shape[0]+15, ('shape: ' + str(center_img_orig.shape)))\n",
    "plt.text(0, center_img_orig.shape[0]+30, ('steering angle: ' + center_steer))\n",
    "plt.imshow(center_img_orig, cmap=\"gray\")\n",
    "\n",
    "plt.subplot2grid((1, 3), (0, 2));\n",
    "plt.axis('off')\n",
    "plt.title('right camera')\n",
    "plt.text(0, right_img_orig.shape[0]+15, ('shape: ' + str(right_img_orig.shape)))\n",
    "plt.imshow(right_img_orig, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Driving Images: Initial Observations\n",
    "We can see from the images above that:\n",
    "- the images are taken in the front of the car (no side or rear angles)\n",
    "- each image is 160x320 with 3 RGB color channels\n",
    "- there is quite a bit of superfluous data, i.e. data that won't benefit the model; for example the sky, hills, trees in the background, as well as the hood of the car).\n",
    "\n",
    "If you view enough images or actually run the simulator, you also see that:\n",
    "- there are a lot of turns in the road (duh!), but since the track ultimately ends where it started, there seems to be more turns in one direction than the other\n",
    "- the lane markings change shape and color at different points in the track, and at some points there are no markings at all!\n",
    "- all of the images are consistently bright; no glare, no darkness, and no shadows that you'd usually encounter with normal driving\n",
    "\n",
    "Given the simulation takes place on a race track (not a highway), the road is free of additional cars, traffic signs, lanes, etc. We won't account for these in this project, but a more robust driving model would need training data that included these conditions. \n",
    "\n",
    "That said, many of the other items above can create biases in our model and cause it to overfit the specific driving conditions within this particular simulation. We need to correct for these so that our model learns to drive in a variety of conditions we might find on other tracks. We'll do this by pre-processing and augmenting our training data throughout the sections to follow. But first let's look at our target data (steering angles) to see if there's anything else we need to correct for. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the steering angles\n",
    "The steering angles are our target data for training the model. That is, based on the images fed into the model while the car is driving along the track, the model will predict the appropriate steering angle to navigate the patch of road ahead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Steering angle distribution function\n",
    "\n",
    "def show_dist(angles):\n",
    "    angles = np.array(angles)\n",
    "    num_bins = 35    \n",
    "    avg_per_bin = len(angles) / num_bins\n",
    "    \n",
    "    print('Total records:', angles.shape[0])\n",
    "    print('Avg per bin: {:0.1f}'.format(avg_per_bin))\n",
    "    \n",
    "    hist, bins = np.histogram(angles, num_bins)\n",
    "    width = 0.8 * (bins[1] - bins[0])\n",
    "    center = (bins[:-1] + bins[1:]) / 2\n",
    "\n",
    "    plt.title('Distribution of Steering Angles')\n",
    "    plt.bar(center, hist, align='center', width=width)\n",
    "    plt.plot((np.min(angles), np.max(angles)), (avg_per_bin, avg_per_bin), 'k-')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution &mdash; Udacity Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "angles = udacity_df.steering.tolist()\n",
    "show_dist(angles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steering Angle Observations**:\n",
    "\n",
    "We can see from the graph above that an overwhelming amount of the target data are steering angles close to zero (i.e., when the car is driving straight). This biases our model to drive the car straight and make it's difficult to learn how to navigate turns. \n",
    "\n",
    "It also seems there may be an imbalance in left vs. right turning data (although not as much as I had expected). It's not clear how big of an impact this would have on the model, but there's a chance the model could learn to turn more effectively in one direction than the other. Just to be safe we'll correct for this by ensuring there are equal quantities of left and right steering data after pre-processing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution &mdash; Self-generated data + Udacity data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "angles = track1_df.steering.tolist()\n",
    "show_dist(angles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Step 2: Data pre-processing\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 | Overview of Data Sources and Splits\n",
    "\n",
    "**Data Sources**: \n",
    "- There are two tracks, but all of the original training and validation data is generated by driving the simulator on **Track 1**.\n",
    "- Udacity provides an intial set of \"good\" data to get us started.\n",
    "- Additional data is gathered by running the simulator myself. \n",
    "\n",
    "**Training Data**:\n",
    "- The training data set includes the original image data captured from the simulator on Track 1, plus any additional data generated via pre-processing and augmentation. \n",
    "\n",
    "**Validation Data**:\n",
    "- The validation data will contain the original image data captured from the simulator on Track 1, with only a pre-process to create a more balanced distribution (i.e., reduce the 'drive straight' bias). No other pre-processing or augmentation is included. This ensures we can properly validate whether changes we're making to the model or training data are increasing or decreasing the model's performance. \n",
    "\n",
    "**Test Data**:\n",
    "- We'll test the model's ability to generalize by running it on **Track 2**. So, the simulator input images from Track 2 will serve as our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Define data sources and their corresponding image directory\n",
    "\n",
    "sources = ['udacity', 'self', 'track1']    # available data sources/groupings\n",
    "source = sources[2]                        # source used for building the pipeline below\n",
    "\n",
    "## Creates correct directory based on the image source\n",
    "def get_img_dir(source):\n",
    "    return \"data/\" + source + \"/IMG/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 | Data Hygiene\n",
    "\n",
    "Before going any further there are few aspects of the data we need to cleanup to make the data easier to work with.\n",
    "\n",
    "1. We're only using the steering data to train the model, so we can prune out the other measurements (throttle, brake, and speed).\n",
    "2. Remove the directory from image path data. We'll be moving our data around and we only want the image filename. \n",
    "3. Cast all of the steering data as floats. In the .csv they're cast as strings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Hygiene function\n",
    "\n",
    "# column references for source data: \n",
    "# 0=center_img, 1=left_img, 2=right_img, 3=steering, 4=throttle, 5=brake, 6=speed\n",
    "\n",
    "def clean(source_data):\n",
    "    '''Performs basic hygiene functions listed above.\n",
    "    \n",
    "    Arguments:\n",
    "    source_data: source data in list format with header row\n",
    "    '''\n",
    "    data_clean = []\n",
    "\n",
    "    for row in source_data[1:]:\n",
    "        # Remove directory from image paths\n",
    "        center = row[0].split('/')[-1]\n",
    "        left = row[1].split('/')[-1]\n",
    "        right = row[2].split('/')[-1]\n",
    "\n",
    "        # Only grab the steering data and cast as float\n",
    "        angle = float(row[3])\n",
    "\n",
    "        data_clean.append([center, left, right, angle])\n",
    "    \n",
    "    return data_clean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Track 1 data hygiene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "track1_clean = clean(track1_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of records:  18605\n",
      "\n",
      "first 3 records:\n",
      " [['center_2016_12_01_13_30_48_287.jpg', 'left_2016_12_01_13_30_48_287.jpg', 'right_2016_12_01_13_30_48_287.jpg', 0.0], ['center_2016_12_01_13_30_48_404.jpg', 'left_2016_12_01_13_30_48_404.jpg', 'right_2016_12_01_13_30_48_404.jpg', 0.0], ['center_2016_12_01_13_31_12_937.jpg', 'left_2016_12_01_13_31_12_937.jpg', 'right_2016_12_01_13_31_12_937.jpg', 0.0]]\n"
     ]
    }
   ],
   "source": [
    "print('number of records: ', len(track1_clean))\n",
    "print('\\nfirst 3 records:\\n', track1_clean[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 | Seperate C/L/R camera data and adjust steering angles for left and right turns\n",
    "Right now, each row of the data set contains three camera angles (center, left, right) and one steering angle which pertains to the center camera. In order to utilize all of the different camera data, we need to: \n",
    "\n",
    "1. Separate the data for each of the different camera angles (only one camera angle per row)\n",
    "2. Adjust the steering angles for the left and right cameras _while the car is turning_. This will compensate for their respective vantage points relative to the center of the car. That is, the steering angle for a right turn should be sharper from the persective of the left camera (and vice versa). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "turn_thresh = 0.15   # the angle threshold used to identify left and right turns\n",
    "ang_corr = np.random.uniform(0.26, 0.28)   # the steering angle correction for left and right cameras\n",
    "\n",
    "\n",
    "def steer_adj(angle):\n",
    "    '''Calculates the absolute value of the steering angle correction for images from \n",
    "    the left and right cameras.\n",
    "    '''\n",
    "    new_angle = min((abs(angle)+ang_corr), 1.0)\n",
    "    \n",
    "    return new_angle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Function for adding left and right camera angles\n",
    "\n",
    "def split_3cam(clean_data, keep_prob):\n",
    "    '''Creates a list of images and angles for all three camera angles. \n",
    "    \n",
    "    Image filenames (inputs) are replaced with actual images to make downstream tranformations easier. \n",
    "    \n",
    "    Arguments:\n",
    "    clean_data: a list of image filenames and angles\n",
    "    keep_prob: probability of keeping zero steering angle records \n",
    "    \n",
    "    Returns:\n",
    "    a list of tuples [(image array, angle)]\n",
    "    \n",
    "    '''\n",
    "    data_3cam = []\n",
    "\n",
    "    for row in clean_data:\n",
    "        \n",
    "        # Convert filenames to images\n",
    "        img_center = mpimg.imread(get_img_dir(source) + row[0])\n",
    "        img_left = mpimg.imread(get_img_dir(source) + row[1])\n",
    "        img_right = mpimg.imread(get_img_dir(source) + row[2])\n",
    "        ang_center = row[3]\n",
    "        \n",
    "        # capture right turn data\n",
    "        if ang_center > turn_thresh:\n",
    "            # center camera, orig steering angle\n",
    "            data_3cam.append([img_center, ang_center]) \n",
    "\n",
    "            # left camera, adjusted steering angle\n",
    "            img_left, ang_left = row[1], steer_adj(row[3])\n",
    "            data_3cam.append([img_left, ang_left])\n",
    "\n",
    "        # capture left turn data\n",
    "        elif ang_center < -turn_thresh:\n",
    "            # center camera, orig steering angle\n",
    "            data_3cam.append([img_center, ang_center]) \n",
    "\n",
    "            # right camera, adjusted steering angle\n",
    "            img_right, ang_right = row[1], -steer_adj(row[3])  \n",
    "            data_3cam.append([img_right, ang_right])\n",
    "\n",
    "        # capture straight driving data\n",
    "        else: \n",
    "            if ang_center == 0:\n",
    "                if np.random.rand() <= keep_prob:\n",
    "                    data_3cam.append([img_center, ang_center])\n",
    "            else:\n",
    "                data_3cam.append([img_center, ang_center])\n",
    "    \n",
    "    return data_3cam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dataset with all 3 cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 22747\n",
      "Avg per bin: 649.9\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEICAYAAAC0+DhzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f02dec7b0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "track1_3cam = split_3cam(track1_clean, 1.0)\n",
    "angles = [i[1] for i in track1_3cam]\n",
    "show_dist(angles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flatten the distribution\n",
    "As we can see from the graph above, there's still MUCH more 'drive straight' data than turning data. So, below we remove most of the zero angle data to somewhat normalize the distribution. We'll actually equalize the distribution later on. For now, we just want to make it more balanced and get a more detailed look at the other parts of the distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "track1_3cam = split_3cam(track1_clean, 0.3)\n",
    "angles = [i[1] for i in track1_3cam]\n",
    "show_dist(angles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 | Crop the images\n",
    "There is a lot of background information in the image that isn't useful for training the model (e.g. trees, sky, birds). Cropping the image helps reduce the level of noise so the model can more easily identify the most important features, namely the turns in the road directly ahead.\n",
    "\n",
    "**NOTE:**\n",
    "The crop function is not actually applied at this point in the pipeline. It happens via the `Cropping2D()` layer near the beginning of the model, just after normalization. We do this there because we also want to crop the input images from the simulator during testing. But, for visualization purposes in this notebook, we'll start showing images in their cropped formed now...so that we can see how other transformations we're making will affect what the model \"sees\" later in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig_shape:  (160, 320, 3)\n",
      "crop_shape:  (80, 280, 3)\n",
      "crop points (h1, h2, w1, w2) = (60, 140, 20, 300)\n"
     ]
    }
   ],
   "source": [
    "## Define crop points\n",
    "\n",
    "# Crop settings provided to Keras Cropping2D layer within the model\n",
    "crop_set = (60, 20), (20, 20)   # number of pixels to remove from (top, bottom), (left, right)\n",
    "    \n",
    "# Model input_shape (160, 320, 3) \n",
    "orig_shape = center_img_orig.shape  \n",
    "\n",
    "# Image shape after cropping\n",
    "crop_shape = (\n",
    "    orig_shape[0]-(crop_set[0][0]+crop_set[0][1]), \\\n",
    "    orig_shape[1]-(crop_set[1][0]+crop_set[1][1]),  \\\n",
    "    orig_shape[2]\n",
    ")\n",
    "\n",
    "# Resulting crop points (for previewing images in notebook)\n",
    "h1, h2 = (crop_set[0][0], orig_shape[0]-crop_set[0][1])\n",
    "w1, w2 = (crop_set[1][0], orig_shape[1]-crop_set[1][1])\n",
    "crop_points = (h1, h2, w1, w2)\n",
    "\n",
    "print('orig_shape: ', orig_shape)\n",
    "print('crop_shape: ', crop_shape)\n",
    "print('crop points (h1, h2, w1, w2) = {}'.format(crop_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Cropping function (again, for notebook display purposes only)\n",
    "\n",
    "def crop(image_data):\n",
    "    h1, h2, w1, w2 = crop_points\n",
    "    \n",
    "    if isinstance(image_data, str):\n",
    "        img_crop = mpimg.imread(get_img_dir(source) + image_data)[h1:h2,w1:w2]\n",
    "    else:\n",
    "        img_crop = image_data[h1:h2,w1:w2]\n",
    "    \n",
    "    return img_crop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 | Flip the images\n",
    "This function was originally executed later in the pipeline within the generator. But, I decided to move it up to the pre-processing stage since it inherently adds more images while balancing the steering angle distribution. Doing this earlier in the process helps prevent some data loss when we attempt to equalize the distribution in the final step of pre-processing. The equalization step (randomly) removes a significant chunk of data, so it's better to add the flipped images beforehand as it increases the chances that at least one version of an image is included in the training data (i.e., it adds more samples and greater variety to our training set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Flip functions\n",
    "\n",
    "\n",
    "# For flipping batches of images and angles input as a list\n",
    "\n",
    "def flip_n(data_3cam):\n",
    "    '''Creates a flipped version of every input image and angle. \n",
    "    \n",
    "    Returns both the original and the flipped versions in list form. \n",
    "    '''\n",
    "    image_names = [i[0] for i in data_3cam]\n",
    "    angles = [i[1] for i in data_3cam]\n",
    "    \n",
    "    images_flip = []\n",
    "    angles_flip = []\n",
    "\n",
    "    for i in range(len(angles)):\n",
    "        # Grab the image\n",
    "        image = mpimg.imread(get_img_dir(source) + image_names[i])\n",
    "        \n",
    "        # Append the original image and angle\n",
    "        images_flip.append(image)\n",
    "        angles_flip.append(angles[i])\n",
    "        \n",
    "        # Append the flipped versions\n",
    "        images_flip.append(cv2.flip(image, 1))\n",
    "        angles_flip.append(-angles[i])\n",
    "\n",
    "    return list(zip(images_flip, angles_flip))\n",
    "\n",
    "\n",
    "# Flips a single image input as either a string or array\n",
    "\n",
    "def flip(image_data):\n",
    "    if isinstance(image_data, str):\n",
    "        image = mpimg.imread(get_img_dir(source) + image_data)\n",
    "    else:\n",
    "        image = image_data\n",
    "        \n",
    "    return np.array(cv2.flip(image, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preview Images: Cropped & Flipped "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Preview cropped and flipped images\n",
    "\n",
    "index = random.randint(0, len(track1_clean))\n",
    "\n",
    "# Select a random set of images to crop\n",
    "center_img_crop = crop(track1_clean[index][0])\n",
    "left_img_crop = crop(track1_clean[index][1])\n",
    "right_img_crop = crop(track1_clean[index][2])\n",
    "\n",
    "# Create flipped versions\n",
    "center_img_flip = flip(center_img_crop)\n",
    "left_img_flip = flip(left_img_crop)\n",
    "right_img_flip = flip(right_img_crop)\n",
    "\n",
    "# Calculate steering angles\n",
    "center_steer = float(track1_clean[index][3])\n",
    "left_steer = None\n",
    "left_steer_flip = None \n",
    "right_steer = None\n",
    "right_steer_flip = None\n",
    "if center_steer > turn_thresh:\n",
    "    left_steer = steer_adj(center_steer)\n",
    "    left_steer_flip = -steer_adj(center_steer)\n",
    "if center_steer < -turn_thresh:\n",
    "    right_steer = -steer_adj(center_steer)\n",
    "    right_steer_flip = steer_adj(center_steer)\n",
    "    \n",
    "# Display visualizations in the notebook\n",
    "plt.figure(figsize=(20,6))\n",
    "\n",
    "# Cropped versions\n",
    "plt.subplot2grid((2, 3), (0, 0));\n",
    "plt.axis('off')\n",
    "plt.title('Left Camera (cropped)')\n",
    "plt.text(0, left_img_crop.shape[0]+15, ('shape: ' + str(left_img_crop.shape)))\n",
    "plt.text(0, left_img_crop.shape[0]+30, ('steering angle: ' + str(left_steer)))\n",
    "plt.imshow(left_img_crop, cmap=\"gray\")\n",
    "\n",
    "plt.subplot2grid((2, 3), (0, 1));\n",
    "plt.axis('off')\n",
    "plt.title('Center Camera (cropped)')\n",
    "plt.text(0, center_img_crop.shape[0]+15, ('shape: ' + str(center_img_crop.shape)))\n",
    "plt.text(0, center_img_crop.shape[0]+30, ('steering angle: ' + str(center_steer)))\n",
    "plt.imshow(center_img_crop, cmap=\"gray\")\n",
    "\n",
    "plt.subplot2grid((2, 3), (0, 2));\n",
    "plt.axis('off')\n",
    "plt.title('Right Camera (cropped)')\n",
    "plt.text(0, right_img_crop.shape[0]+15, ('shape: ' + str(right_img_crop.shape)))\n",
    "plt.text(0, right_img_crop.shape[0]+30, ('steering angle: ' + str(right_steer)))\n",
    "plt.imshow(right_img_crop, cmap=\"gray\")\n",
    "\n",
    "# Flipped version\n",
    "plt.subplot2grid((2, 3), (1, 0));\n",
    "plt.axis('off')\n",
    "plt.title('Left Camera (cropped + flipped)')\n",
    "plt.text(0, left_img_flip.shape[0]+15, ('shape: ' + str(left_img_flip.shape)))\n",
    "plt.text(0, left_img_flip.shape[0]+30, ('steering angle: ' + str(left_steer_flip)))\n",
    "plt.imshow(left_img_flip, cmap=\"gray\")\n",
    "\n",
    "plt.subplot2grid((2, 3), (1, 1));\n",
    "plt.axis('off')\n",
    "plt.title('Center Camera (cropped + flipped)')\n",
    "plt.text(0, center_img_flip.shape[0]+15, ('shape: ' + str(center_img_flip.shape)))\n",
    "plt.text(0, center_img_flip.shape[0]+30, ('steering angle: ' + str(-center_steer)))\n",
    "plt.imshow(center_img_flip, cmap=\"gray\")\n",
    "\n",
    "plt.subplot2grid((2, 3), (1, 2));\n",
    "plt.axis('off')\n",
    "plt.title('Right Camera (cropped + flipped)')\n",
    "plt.text(0, right_img_flip.shape[0]+15, ('shape: ' + str(right_img_flip.shape)))\n",
    "plt.text(0, right_img_flip.shape[0]+30, ('steering angle: ' + str(right_steer_flip)))\n",
    "plt.imshow(right_img_flip, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add flipped images to dataset\n",
    "Again, we're going to crop them here for demonstration purposes, but ultimately cropping will be done in the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "track1_crop = [(crop(i[0]), i[1]) for i in track1_3cam]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Count of cropped images:', len(track1_crop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create flip version with cropping\n",
    "\n",
    "# track1_flip = [(flip(i[0]), -i[1]) for i in track1_crop] + track1_crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create flip version WITHOUT cropping\n",
    "\n",
    "track1_flip = [(flip(i[0]), -i[1]) for i in track1_3cam] + track1_3cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45494"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(track1_flip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution with addition of flipped images\n",
    "We can see that the nuber of left and right turns in each bin is now perfectly balanced. And, we now have twice as many training images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 45494\n",
      "Avg per bin: 1299.8\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEICAYAAAC0+DhzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJPUYDpKkHsNBktRjOEiSegwHSVLP/wdThU3jQLkmXAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f02db6d2630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "angles = [i[1] for i in track1_flip]\n",
    "show_dist(angles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 | Equalize the Distribution\n",
    "We want our model to get roughly the same number of training samples for each bin of steering angles. This will ensure that the model isn't better at steering in one direction versus another (or not steering at all!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: I borrowed the function below from Jeremy Shannon ([source code](https://github.com/jeremy-shannon/CarND-Behavioral-Cloning-Project/blob/master/model.py#L225)) with a few modifications. Great to see other Udacity students publishing such useful tools. Jeremy's code was better than anything else I found on Stack Overflow etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def equal_dist(data, factor):\n",
    "    '''Creates a more equalized distribution of steering angles.\n",
    "    \n",
    "    Basic logic:\n",
    "    - If the number of samples in a given bin is below the target number, keep all samples for that bin.\n",
    "    - Otherwise the keep prob for that bin is set to bring the number of samples for that bin down to the average.\n",
    "    \n",
    "    '''\n",
    "    images = [i[0] for i in data]\n",
    "    angles = [i[1] for i in data]\n",
    "    \n",
    "    num_bins = 35\n",
    "    avg_samples_per_bin = len(angles) / num_bins\n",
    "    target = avg_samples_per_bin * factor\n",
    "\n",
    "    hist, bins = np.histogram(angles, num_bins)\n",
    "    \n",
    "    # Determine keep probability for each bin\n",
    "    keep_probs = []\n",
    "    for i in range(num_bins):\n",
    "        if hist[i] <= target:\n",
    "            keep_probs.append(1.)\n",
    "        else:\n",
    "            keep_prob = 1./(hist[i]/target)\n",
    "            keep_probs.append(keep_prob)\n",
    "    \n",
    "    # Create list of angles to remove because bin count is above the target\n",
    "    remove_list = []\n",
    "    for i in range(len(angles)):\n",
    "        for j in range(num_bins):\n",
    "            if angles[i] >= bins[j] and angles[i] <= bins[j+1]:\n",
    "                # delete with probability 1 - keep_probs[j]\n",
    "                if np.random.rand() > keep_probs[j]:\n",
    "                    remove_list.append(i)\n",
    "\n",
    "    for i in sorted(remove_list, reverse=True):\n",
    "        del images[i]\n",
    "        del angles[i]\n",
    "        \n",
    "    eq_data = []\n",
    "    \n",
    "    for i in range(len(angles)):\n",
    "        eq_data.append((images[i], angles[i]))\n",
    "    \n",
    "    return eq_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Current Distribution &mdash; Track 1 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "track1_angles = np.array([i[1] for i in track1_flip])\n",
    "show_dist(track1_angles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New Distribution &mdash; Track 1 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 16884\n",
      "Avg per bin: 482.4\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f02de544630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'track1_angles' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-9e266e22e55c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrack1_angles_eq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrack1_eq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mshow_dist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack1_angles_eq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Records removed:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack1_angles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack1_angles_eq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'track1_angles' is not defined"
     ]
    }
   ],
   "source": [
    "track1_eq = equal_dist(track1_flip, 0.5)\n",
    "track1_angles_eq = [i[1] for i in track1_eq]\n",
    "show_dist(track1_angles_eq)\n",
    "print('Records removed:', len(track1_angles)-len(track1_angles_eq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 | Image Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reviewing the various smoothing techniques discussed [here in the OpenCV docs](http://docs.opencv.org/3.1.0/d4/d13/tutorial_py_filtering.html), I decided to use `cv2.bilateralFilter()`. While the operation is slower than the other filters, it has the advantage of removing noise from the image **while preserving the edges**. A more in depth discussion on bilateral filtering can be found [here](http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/MANDUCHI1/Bilateral_Filtering.html) (University of Edinburgh). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def img_smooth(image_data):\n",
    "    blur_filter = (5, 80, 80)\n",
    "    \n",
    "    if isinstance(image_data, str):\n",
    "        image = mpimg.imread(get_img_dir(source) + image_data)\n",
    "    else:\n",
    "        image = image_data\n",
    "    \n",
    "    img_blur = cv2.bilateralFilter(image, blur_filter[0], blur_filter[1], blur_filter[2])\n",
    "    \n",
    "    return img_blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Preview smoothed images\n",
    "\n",
    "index = random.randint(0, len(track1_clean))\n",
    "\n",
    "# Select a random set of images to crop\n",
    "center_img_crop = crop(track1_clean[index][0])\n",
    "left_img_crop = crop(track1_clean[index][1])\n",
    "right_img_crop = crop(track1_clean[index][2])\n",
    "\n",
    "# Create smoothed versions\n",
    "center_img_blur = img_smooth(center_img_crop)\n",
    "left_img_blur = img_smooth(left_img_crop)\n",
    "right_img_blur = img_smooth(right_img_crop)\n",
    "    \n",
    "# Display visualizations in the notebook\n",
    "plt.figure(figsize=(20,6))\n",
    "\n",
    "# Cropped versions\n",
    "plt.subplot2grid((2, 3), (0, 0));\n",
    "plt.axis('off')\n",
    "plt.title('Left Camera')\n",
    "plt.imshow(left_img_crop, cmap=\"gray\")\n",
    "\n",
    "plt.subplot2grid((2, 3), (0, 1));\n",
    "plt.axis('off')\n",
    "plt.title('Center Camera')\n",
    "plt.imshow(center_img_crop, cmap=\"gray\")\n",
    "\n",
    "plt.subplot2grid((2, 3), (0, 2));\n",
    "plt.axis('off')\n",
    "plt.title('Right Camera')\n",
    "plt.imshow(right_img_crop, cmap=\"gray\")\n",
    "\n",
    "# Flipped version\n",
    "plt.subplot2grid((2, 3), (1, 0));\n",
    "plt.axis('off')\n",
    "plt.title('Left Camera (smoothed)')\n",
    "plt.imshow(left_img_blur, cmap=\"gray\")\n",
    "\n",
    "plt.subplot2grid((2, 3), (1, 1));\n",
    "plt.axis('off')\n",
    "plt.title('Center Camera (smoothed)')\n",
    "plt.imshow(center_img_blur, cmap=\"gray\")\n",
    "\n",
    "plt.subplot2grid((2, 3), (1, 2));\n",
    "plt.axis('off')\n",
    "plt.title('Right Camera (smoothed)')\n",
    "plt.imshow(right_img_blur, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Apply smoothing to dataset\n",
    "The above preview looks good. The image has ample noise reduction with very little blurring of the edges. So, we can now apply this effect to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "track1_blur = [(img_smooth(i[0]), i[1]) for i in track1_eq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Verify smoothing effect\n",
    "\n",
    "index = random.randint(0, len(track1_blur))\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.axis('off')\n",
    "plt.imshow(track1_blur[index][0], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 |  Pre-Process the Data Set\n",
    "Here we apply some of all of the pre-processing functions described above in order to create our pre-processed data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def pre_process(source_data):\n",
    "    \n",
    "#     clean_data = clean(source_data)\n",
    "#     cam3_data = split_3cam(clean_data, 0.1)\n",
    "#     flip_data = [(flip(i[0]), -i[1]) for i in cam3_data] + cam3_data\n",
    "#     blur_data = [(img_smooth(i[0]), i[1]) for i in flip_data]\n",
    "#     pp_images, pp_angles = equal_dist(blur_data, 0.5)\n",
    "    \n",
    "#     return pp_images, pp_angles\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run through each pre-processing step separately\n",
    "Running them together as part of single function seems to crash my notebook (i.e., dead kernel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clean the data\n",
    "clean_data = clean(track1_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the data to capture Left, Right, and Center camera angles\n",
    "cam3_data = split_3cam(clean_data, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check number of records\n",
    "len(cam3_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add a flipped version of each image and angle\n",
    "flip_data = [(flip(i[0]), -i[1]) for i in cam3_data] + cam3_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check number of records - should be 2x the 3cam output\n",
    "len(flip_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Equalize the distribution to diminish the bias toward straight driving\n",
    "eq_data = equal_dist(flip_data, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check number of records - should be less than flip_data output\n",
    "print('Total Records: ', len(eq_data))\n",
    "print('Records removed:', len(flip_data)-len(eq_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TEMPORARY - FOR TESTING\n",
    "\n",
    "# Smooth the images using a bilateral filter - this is compute intensive, so we do it after pruning\n",
    "blur_data = [(img_smooth(i[0]), i[1]) for i in track1_eq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Smooth the images using a bilateral filter - this is compute intensive, so we do it after pruning\n",
    "# blur_data = [(img_smooth(i[0]), i[1]) for i in eq_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create list of pre-processed images\n",
    "pp_images = [i[0] for i in blur_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create list of pre-processed angles\n",
    "pp_angles = [i[1] for i in blur_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16884"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pp_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16884"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pp_angles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline status check &mdash; Where are we at?\n",
    "We've done a lot already. Let's take a minute to see where we're at. There are many steps in the pipeline, so it's easy to get lost. \n",
    "\n",
    "<br>\n",
    "**Completed Steps** &mdash; Here's what we've done so far:\n",
    "\n",
    "**1** - Load and examine the data <br>\n",
    "**2** - Pre-process the data to make it easier for the model to extract the most important features <br>\n",
    "<br>\n",
    "<br>\n",
    "**Next Steps** &mdash; In the subsequent parts of the pipeline, we will:\n",
    "\n",
    "**3** - Split the data into training and validation sets <br>\n",
    "**4** - Feed the training sets into a generator to undergoe further augmentation (e.g. random brightness and affine transformations). This step is not applied to the validation data. <br>\n",
    "**5** - Feed the augmented training data into the model where it is normalized, cropped, and resized <br>\n",
    "**6** - Train the model on the augmented data set <br>\n",
    "**7** - Test the model on the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Training and Validation sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples:  13507\n",
      "Number of validation examples:  3377\n",
      "-------\n",
      "Verify Totals: 16884 = 16884\n"
     ]
    }
   ],
   "source": [
    "## Create training and validation sets\n",
    "\n",
    "X_train = list(pp_images)     # list of pre-processed image arrays\n",
    "y_train = list(pp_angles)     # list of pre-processed angles\n",
    "\n",
    "# Split training and testing data\n",
    "X_train, X_valid, y_train, y_valid = \\\n",
    "                train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "# Number of training examples\n",
    "n_train = len(X_train)\n",
    "\n",
    "# Number of validation examples\n",
    "n_valid = len(X_valid)\n",
    "\n",
    "# Verify that all counts match\n",
    "print('Number of training examples: ', n_train)\n",
    "print('Number of validation examples: ', n_valid)\n",
    "print('-------\\nVerify Totals: {} = {}'.format(len(pp_images), (n_train+n_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ## Create training and validation sets\n",
    "\n",
    "# X_train_self = [(i[0], i[2]) for i in self_3cam]   # produces list of tuples [(image filename, source)]\n",
    "# y_train_self = [i[1] for i in self_3cam]           # produces list of angles\n",
    "\n",
    "# # Split training and testing data\n",
    "# X_train_self, X_valid_self, y_train_self, y_valid_self = \\\n",
    "#                 train_test_split(X_train_self, y_train_self, test_size=0.2, random_state=0)\n",
    "\n",
    "# # Split out source data\n",
    "# src_train_self = [i[1] for i in X_train_self]\n",
    "# X_train_self = [i[0] for i in X_train_self]\n",
    "\n",
    "# src_valid_self = [i[1] for i in X_valid_self]\n",
    "# X_valid_self = [i[0] for i in X_valid_self]\n",
    "\n",
    "\n",
    "# # Number of training examples\n",
    "# n_train_self = len(X_train_self)\n",
    "\n",
    "# # Number of validation examples\n",
    "# n_valid_self = len(X_valid_self)\n",
    "\n",
    "# # Number of corresponding sources\n",
    "# n_src_train_self = len(src_train_self)\n",
    "# n_src_valid_self = len(src_valid_self)\n",
    "\n",
    "# # Verify that all counts match\n",
    "# print(\"Number of training examples: \", n_train_self)\n",
    "# print(\"Number of validation examples: \", n_valid_self)\n",
    "# print(\"Source counts: {} = {}, and {} = {} \".format(n_src_train_self, n_train_self, n_src_valid_self, n_valid_self))\n",
    "# print(\"----------\\nVerify Totals: {} = {} \".format((n_train_self+n_valid_self), len(self_3cam)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Data Generator "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 | Change Brightness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generates batches of tensor image data that is augmented based on a chosen set of tranformation parameters (e.g. rotation, shift, shear, zoom)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# datagen_3 = ImageDataGenerator(featurewise_center=False,\n",
    "#     samplewise_center=False,\n",
    "#     featurewise_std_normalization=False,\n",
    "#     samplewise_std_normalization=False,\n",
    "#     zca_whitening=True,\n",
    "#     zca_epsilon=1e-6,\n",
    "#     rotation_range=2.,\n",
    "#     width_shift_range=0.1,\n",
    "#     height_shift_range=0.1,\n",
    "#     shear_range=0.01,\n",
    "#     zoom_range=0.,\n",
    "#     channel_shift_range=0.1,\n",
    "#     fill_mode='nearest',\n",
    "#     cval=0.,\n",
    "#     horizontal_flip=False,\n",
    "#     vertical_flip=False,\n",
    "#     rescale=None,\n",
    "#     preprocessing_function=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Randomly shift brightness\n",
    "\n",
    "def brightness(image):\n",
    "    # Convert to HSV from RGB \n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "    \n",
    "    # Generate random brightness\n",
    "    rand = random.uniform(0.6, 1)\n",
    "    hsv[:,:,2] = rand*hsv[:,:,2]\n",
    "    \n",
    "    # Convert back to RGB \n",
    "    new_img = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n",
    "    \n",
    "    return new_img "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Randomly shift horizon\n",
    "\n",
    "def horizon(image):\n",
    "    h, w, _ = image.shape\n",
    "    horizon = 2*h/5\n",
    "    v_shift = np.random.randint(-h/8,h/8)\n",
    "    pts1 = np.float32([[0,horizon],[w,horizon],[0,h],[w,h]])\n",
    "    pts2 = np.float32([[0,horizon+v_shift],[w,horizon+v_shift],[0,h],[w,h]])\n",
    "    M = cv2.getPerspectiveTransform(pts1,pts2)\n",
    "    new_img = cv2.warpPerspective(image,M,(w,h), borderMode=cv2.BORDER_REPLICATE)\n",
    "    \n",
    "    return new_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combine the brightness and horizon functions so they can be executed within the Keras ImageDataGenerator\n",
    "\n",
    "def bright_horizon(image):\n",
    "    bright_img = brightness(image)\n",
    "    new_img = horizon(bright_img)\n",
    "    \n",
    "    return new_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Image transformation function\n",
    "\n",
    "image = np.empty((160, 320, 3), dtype='uint8')\n",
    "\n",
    "keras_datagen = ImageDataGenerator(\n",
    "#     zca_whitening=True,\n",
    "    rotation_range=1,\n",
    "    width_shift_range=0.02,\n",
    "    height_shift_range=0.02,\n",
    "#     shear_range=0.05,\n",
    "    zoom_range=0.02,\n",
    "#     channel_shift_range=10.0,\n",
    "    fill_mode='nearest',\n",
    "    preprocessing_function=bright_horizon\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'udacity_3cam' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-1b6188648c96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Create AUGMENTED training sets **WORKING VERSION**\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mudacity_3cam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mudacity_3cam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mudacity_3cam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'udacity_3cam' is not defined"
     ]
    }
   ],
   "source": [
    "## Create AUGMENTED training sets **WORKING VERSION**\n",
    "\n",
    "sample = random.sample(range(1, len(udacity_3cam)), 16)\n",
    "\n",
    "X_batch = [(udacity_3cam[i][0], udacity_3cam[i][2]) for i in sample]\n",
    "y_batch = [udacity_3cam[i][1] for i in sample]\n",
    "\n",
    "X_images = np.empty((0, 160, 320, 3), dtype='uint8')\n",
    "y_angles = np.empty(0, dtype='float32')\n",
    "\n",
    "source = default_source\n",
    "\n",
    "for i in range(len(y_batch)):\n",
    "    # retrieve the image from local directory\n",
    "    source = str(X_batch[i][1])\n",
    "    filename = str(X_batch[i][0])\n",
    "    img_path = get_img_dir(source) + filename\n",
    "    img = [mpimg.imread(img_path.strip())]\n",
    "    angle = [y_batch[i]]\n",
    "    X_images = np.append(X_images, img, axis=0)\n",
    "    y_angles = np.append(y_angles, angle, axis=0) \n",
    "    \n",
    "X_aug = np.empty((0, 160, 320, 3))\n",
    "y_aug = np.empty(0, dtype='float32')\n",
    "\n",
    "print('Augmenting Image Data...')\n",
    "\n",
    "# seed = random.randint(1, len(y_batch))\n",
    "# datagen.fit(X_images)\n",
    "\n",
    "for X,y in keras_datagen.flow(X_images, y_angles, batch_size=len(y_batch)):       \n",
    "    X_aug = np.append(X_aug, X, axis=0)\n",
    "    y_aug = np.append(y_aug, y, axis=0)\n",
    "    \n",
    "    if len(y_aug) == len(y_batch):\n",
    "        break\n",
    "\n",
    "X_aug = X_aug.astype(np.uint8)\n",
    "\n",
    "print('Augmentation Complete.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Transformation function ** V 2 **\n",
    " \n",
    "def transform(X_batch, y_batch):\n",
    "    '''Applies a random set of transformations which are defined by the \n",
    "    keras_datagen() function and uses the Keras ImageDataGenerator.\n",
    "    \n",
    "    Arguments:\n",
    "    X_batch: a numpy array of images\n",
    "    y_batch: a numpy array of steering angles\n",
    "        \n",
    "    Returns:\n",
    "    X_aug: a numpy array of transformed image data\n",
    "    y_aug: a numpy array of steering angles\n",
    "\n",
    "    '''\n",
    "    X_aug = np.empty((0, 160, 320, 3))\n",
    "    y_aug = np.empty(0, dtype='float32')\n",
    "    \n",
    "#     keras_datagen.fit(X_batch)\n",
    "    \n",
    "    n_aug = 1     # number of augmented images to create for every input image\n",
    "    \n",
    "    for X,y in keras_datagen.flow(X_batch, y_batch, batch_size=len(y_batch)):       \n",
    "        X_aug = np.append(X_aug, X, axis=0)\n",
    "        y_aug = np.append(y_aug, y, axis=0)\n",
    "        \n",
    "        if X_aug.shape[0] == n_aug*X_batch.shape[0]:\n",
    "            break\n",
    "    \n",
    "    X_aug = X_aug.astype(np.uint8)\n",
    "    \n",
    "    return (X_aug, y_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 | Batch Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(images, angles, source, val=False):\n",
    "    '''Generates batches of images to feed into the model. \n",
    "    \n",
    "    For each input image, four different versions are generated:\n",
    "    img_1 : original version\n",
    "    img_2 : flipped version of 1\n",
    "    img_3 : version of 1 with other random transformations (for training only)\n",
    "    img_4 : version of 2 with other random transformations (for training only)\n",
    "    \n",
    "    Arguments:\n",
    "    images: a list of image filenames\n",
    "    angles: a list of angles\n",
    "    source: the original data source, 'udacity' or 'self'\n",
    "    val: whether the data is being generated for validation (False by default)\n",
    "    \n",
    "    Yields: \n",
    "    X_batch: a numpy array of image data\n",
    "    y_batch: a numpy array of steering angles\n",
    "    \n",
    "    '''\n",
    "    images, angles = shuffle(images, angles, random_state=0)\n",
    "    \n",
    "    X_batch = np.empty((0, 160, 320, 3), dtype='uint8')\n",
    "    y_batch = np.empty(0, dtype='float32')\n",
    "    \n",
    "    while True:\n",
    "        for i in range(len(angles)):\n",
    "#             # retrieve the original image from local directory\n",
    "#             img_path = get_img_dir(source) + str(images[i])\n",
    "#             img_path = get_img_dir(source) + images[i]\n",
    "#             img_1 = [mpimg.imread(img_path.strip())]\n",
    "\n",
    "            img_1 = [images[i]]\n",
    "            ang_1 = [angles[i]]\n",
    "            X_batch = np.append(X_batch, img_1, axis=0)\n",
    "            y_batch = np.append(y_batch, ang_1, axis=0)\n",
    "\n",
    "#             # generate flipped version\n",
    "#             img_2 = [cv2.flip(img_1[0], 1)]\n",
    "#             ang_2 = [-angles[i]]\n",
    "#             X_batch = np.append(X_batch, img_2, axis=0)\n",
    "#             y_batch = np.append(y_batch, ang_2, axis=0)\n",
    "            \n",
    "            # augmentation process; for training images only\n",
    "            if not val:\n",
    "                # apply other transformations\n",
    "                imgs_2_3, angs_2_3 = transform(X_batch[-1:], y_batch[-1:])\n",
    "\n",
    "                X_batch = np.append(X_batch, imgs_2_3, axis=0)\n",
    "                y_batch = np.append(y_batch, angs_2_3, axis=0)\n",
    "                \n",
    "            if X_batch.shape[0] >= batch_size:\n",
    "                X_batch, y_batch = shuffle(X_batch[0:batch_size], y_batch[0:batch_size])\n",
    "                yield (X_batch, y_batch)\n",
    "                X_batch = np.empty((0, 160, 320, 3), dtype='uint8')\n",
    "                y_batch = np.empty(0, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Display sample of the ORIGINAL training images\n",
    "\n",
    "fig = plt.figure(figsize=(20,16))\n",
    "\n",
    "orig_images = X_images\n",
    "sample = random.sample(range(len(X_aug)), 16)\n",
    "\n",
    "for i in range(16):\n",
    "    img = orig_images[sample[i]]\n",
    "    ax = fig.add_subplot(5,4,i+1)\n",
    "    ax.imshow(img.squeeze(), cmap=\"gray\", interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Display sample of training images with POSITIONAL SHIFTS (width, height rotation, horizon)\n",
    "\n",
    "fig = plt.figure(figsize=(20,16)) \n",
    "\n",
    "sample = random.sample(range(len(X_aug)), 16)\n",
    "\n",
    "for i in range(16):\n",
    "    img = X_aug[i]\n",
    "    ax = fig.add_subplot(5,4,i+1)\n",
    "    ax.imshow(img.squeeze(), cmap=\"gray\", interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Display sample of training images with BRIGHTNESS SHIFT \n",
    "\n",
    "fig = plt.figure(figsize=(20,16)) \n",
    "\n",
    "sample = random.sample(range(len(X_aug)), 16)\n",
    "\n",
    "for i in range(16):\n",
    "    img = X_aug[i]\n",
    "    ax = fig.add_subplot(5,4,i+1)\n",
    "    ax.imshow(img.squeeze(), cmap=\"gray\", interpolation='nearest')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Model Architecture\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Global variables and parameters\n",
    "\n",
    "lr = 1e-4        # learning rate\n",
    "reg = l2(1e-3)   # L2 reg\n",
    "drop = 0.5       # default dropout rate\n",
    "\n",
    "d_str = (2, 2)     # default strides\n",
    "d_act = 'elu'      # default activation function\n",
    "d_pad = 'same'     # default padding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Model v7 - based on Comma.ai model\n",
    "# https://github.com/commaai/research/blob/master/train_steering_model.py#L24\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Lambda(lambda x: x/255 - 0.5, input_shape=orig_shape))\n",
    "model.add(Cropping2D(cropping=((60, 20), (20, 20))))\n",
    "\n",
    "model.add(Conv2D(16, 8, strides=(4, 4), padding=d_pad, activation=d_act)) # ,  kernel_regularizer=reg)))\n",
    "# model.add(ELU())\n",
    "model.add(Conv2D(32, 5, strides=d_str, padding=d_pad, activation=d_act))\n",
    "# model.add(ELU())\n",
    "model.add(Conv2D(64, 5, strides=d_str, padding=d_pad, activation=d_act))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(.2))\n",
    "model.add(Activation(d_act))\n",
    "model.add(Dense(512))\n",
    "model.add(Dropout(.5))\n",
    "model.add(Activation(d_act))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile and preview the model\n",
    "model.compile(optimizer=Adam(lr=lr), loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ## Comma.ai model\n",
    "# # https://github.com/commaai/research/blob/master/train_steering_model.py#L24\n",
    "\n",
    "# def get_model(time_len=1):\n",
    "#     ch, row, col = 3, 160, 320  # camera format\n",
    "\n",
    "#     model = Sequential()\n",
    "#     model.add(Lambda(lambda x: x/127.5 - 1.,\n",
    "#             input_shape=(ch, row, col),\n",
    "#             output_shape=(ch, row, col)))\n",
    "#     model.add(Convolution2D(16, 8, 8, subsample=(4, 4), border_mode=\"same\"))\n",
    "#     model.add(ELU())\n",
    "#     model.add(Convolution2D(32, 5, 5, subsample=(2, 2), border_mode=\"same\"))\n",
    "#     model.add(ELU())\n",
    "#     model.add(Convolution2D(64, 5, 5, subsample=(2, 2), border_mode=\"same\"))\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dropout(.2))\n",
    "#     model.add(ELU())\n",
    "#     model.add(Dense(512))\n",
    "#     model.add(Dropout(.5))\n",
    "#     model.add(ELU())\n",
    "#     model.add(Dense(1))\n",
    "\n",
    "#     model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Create and reset the model  ** V6 - NVIDIA ** \n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Lambda(lambda x: x/255 - 0.5, input_shape=orig_shape))\n",
    "model.add(Cropping2D(cropping=((60, 20), (20, 20))))\n",
    "\n",
    "model.add(Conv2D(32, 5, strides=strides, padding=default_pad, activation=act,  kernel_regularizer=reg))\n",
    "model.add(Conv2D(48, 5, strides=strides, padding=default_pad, activation=act,  kernel_regularizer=reg))\n",
    "model.add(Conv2D(64, 5, strides=strides, padding=default_pad, activation=act,  kernel_regularizer=reg))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2), padding='valid'))\n",
    "\n",
    "model.add(Conv2D(80, 3, strides=strides, padding=default_pad, activation=act,  kernel_regularizer=reg))\n",
    "model.add(Conv2D(80, 3, strides=strides, padding=default_pad, activation=act,  kernel_regularizer=reg))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2), padding='valid'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation=act,  kernel_regularizer=reg))\n",
    "# model.add(Dropout(drop))\n",
    "model.add(Dense(128, activation=act,  kernel_regularizer=reg))\n",
    "# model.add(Dropout(drop))\n",
    "model.add(Dense(10, activation=act,  kernel_regularizer=reg))\n",
    "# model.add(Dropout(drop))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile and preview the model\n",
    "model.compile(optimizer=Adam(lr=lr), loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_1 (Lambda)            (None, 160, 320, 3)       0         \n",
      "_________________________________________________________________\n",
      "cropping2d_1 (Cropping2D)    (None, 80, 280, 3)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 40, 140, 24)       1824      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 20, 70, 36)        21636     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 10, 35, 48)        43248     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 5, 18, 64)         27712     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 3, 9, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1728)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 150)               259350    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                7550      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 398,769\n",
      "Trainable params: 398,769\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Create and reset the model  ** V1 - NVIDIA ** \n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Lambda(lambda x: x/255 - 0.5, input_shape=orig_shape))\n",
    "model.add(Cropping2D(cropping=((60, 20), (20, 20))))\n",
    "\n",
    "model.add(Conv2D(24, 5, strides=d_str, padding=d_pad, activation=d_act,  kernel_regularizer=reg))\n",
    "model.add(Conv2D(36, 5, strides=d_str, padding=d_pad, activation=d_act,  kernel_regularizer=reg))\n",
    "model.add(Conv2D(48, 5, strides=d_str, padding=d_pad, activation=d_act,  kernel_regularizer=reg))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2), padding='valid'))\n",
    "\n",
    "model.add(Conv2D(64, 3, strides=d_str, padding=d_pad, activation=d_act,  kernel_regularizer=reg))\n",
    "model.add(Conv2D(64, 3, strides=d_str, padding=d_pad, activation=d_act,  kernel_regularizer=reg))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2), padding='valid'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(150, activation=d_act,  kernel_regularizer=reg))\n",
    "# model.add(Dropout(drop))\n",
    "model.add(Dense(50, activation=d_act,  kernel_regularizer=reg))\n",
    "# model.add(Dropout(drop))\n",
    "model.add(Dense(10, activation=d_act,  kernel_regularizer=reg))\n",
    "# model.add(Dropout(drop))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile and preview the model\n",
    "model.compile(optimizer=Adam(lr=lr), loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "211/211 [==============================] - 105s - loss: 0.6315 - acc: 0.0394 - val_loss: 0.5683 - val_acc: 0.0424\n",
      "Epoch 2/75\n",
      "211/211 [==============================] - 99s - loss: 0.5544 - acc: 0.0402 - val_loss: 0.4997 - val_acc: 0.0421\n",
      "Epoch 3/75\n",
      "211/211 [==============================] - 99s - loss: 0.4963 - acc: 0.0418 - val_loss: 0.4619 - val_acc: 0.0424\n",
      "Epoch 4/75\n",
      "211/211 [==============================] - 100s - loss: 0.4467 - acc: 0.0425 - val_loss: 0.4077 - val_acc: 0.0418\n",
      "Epoch 5/75\n",
      "211/211 [==============================] - 100s - loss: 0.4064 - acc: 0.0438 - val_loss: 0.3770 - val_acc: 0.0439\n",
      "Epoch 6/75\n",
      "211/211 [==============================] - 99s - loss: 0.3708 - acc: 0.0447 - val_loss: 0.3356 - val_acc: 0.0475\n",
      "Epoch 7/75\n",
      "211/211 [==============================] - 99s - loss: 0.3417 - acc: 0.0474 - val_loss: 0.3171 - val_acc: 0.0466\n",
      "Epoch 8/75\n",
      "211/211 [==============================] - 99s - loss: 0.3162 - acc: 0.0465 - val_loss: 0.2888 - val_acc: 0.0478\n",
      "Epoch 9/75\n",
      "211/211 [==============================] - 99s - loss: 0.2952 - acc: 0.0476 - val_loss: 0.2736 - val_acc: 0.0481\n",
      "Epoch 10/75\n",
      "211/211 [==============================] - 100s - loss: 0.2742 - acc: 0.0474 - val_loss: 0.2507 - val_acc: 0.0490\n",
      "Epoch 11/75\n",
      "211/211 [==============================] - 101s - loss: 0.2592 - acc: 0.0482 - val_loss: 0.2419 - val_acc: 0.0499\n",
      "Epoch 12/75\n",
      "211/211 [==============================] - 100s - loss: 0.2419 - acc: 0.0478 - val_loss: 0.2237 - val_acc: 0.0490\n",
      "Epoch 13/75\n",
      "211/211 [==============================] - 101s - loss: 0.2293 - acc: 0.0493 - val_loss: 0.2156 - val_acc: 0.0499\n",
      "Epoch 14/75\n",
      "211/211 [==============================] - 100s - loss: 0.2162 - acc: 0.0489 - val_loss: 0.1994 - val_acc: 0.0487\n",
      "Epoch 15/75\n",
      "211/211 [==============================] - 100s - loss: 0.2076 - acc: 0.0489 - val_loss: 0.1906 - val_acc: 0.0496\n",
      "Epoch 16/75\n",
      "211/211 [==============================] - 100s - loss: 0.1953 - acc: 0.0497 - val_loss: 0.1808 - val_acc: 0.0490\n",
      "Epoch 17/75\n",
      "211/211 [==============================] - 100s - loss: 0.1891 - acc: 0.0495 - val_loss: 0.1741 - val_acc: 0.0505\n",
      "Epoch 18/75\n",
      "211/211 [==============================] - 100s - loss: 0.1786 - acc: 0.0501 - val_loss: 0.1661 - val_acc: 0.0502\n",
      "Epoch 19/75\n",
      "211/211 [==============================] - 100s - loss: 0.1738 - acc: 0.0497 - val_loss: 0.1614 - val_acc: 0.0505\n",
      "Epoch 20/75\n",
      "211/211 [==============================] - 100s - loss: 0.1650 - acc: 0.0506 - val_loss: 0.1549 - val_acc: 0.0490\n",
      "Epoch 21/75\n",
      "211/211 [==============================] - 100s - loss: 0.1616 - acc: 0.0499 - val_loss: 0.1492 - val_acc: 0.0502\n",
      "Epoch 22/75\n",
      "211/211 [==============================] - 105s - loss: 0.1541 - acc: 0.0505 - val_loss: 0.1442 - val_acc: 0.0502\n",
      "Epoch 23/75\n",
      "211/211 [==============================] - 105s - loss: 0.1518 - acc: 0.0504 - val_loss: 0.1415 - val_acc: 0.0505\n",
      "Epoch 24/75\n",
      "211/211 [==============================] - 100s - loss: 0.1455 - acc: 0.0512 - val_loss: 0.1339 - val_acc: 0.0502\n",
      "Epoch 25/75\n",
      "211/211 [==============================] - 100s - loss: 0.1442 - acc: 0.0503 - val_loss: 0.1312 - val_acc: 0.0517\n",
      "Epoch 26/75\n",
      "211/211 [==============================] - 100s - loss: 0.1374 - acc: 0.0517 - val_loss: 0.1277 - val_acc: 0.0508\n",
      "Epoch 27/75\n",
      "211/211 [==============================] - 104s - loss: 0.1363 - acc: 0.0507 - val_loss: 0.1255 - val_acc: 0.0517\n",
      "Epoch 28/75\n",
      "211/211 [==============================] - 104s - loss: 0.1318 - acc: 0.0517 - val_loss: 0.1231 - val_acc: 0.0505\n",
      "Epoch 29/75\n",
      "211/211 [==============================] - 107s - loss: 0.1320 - acc: 0.0509 - val_loss: 0.1205 - val_acc: 0.0520\n",
      "Epoch 30/75\n",
      "211/211 [==============================] - 105s - loss: 0.1262 - acc: 0.0517 - val_loss: 0.1149 - val_acc: 0.0508\n",
      "Epoch 31/75\n",
      "211/211 [==============================] - 103s - loss: 0.1272 - acc: 0.0512 - val_loss: 0.1150 - val_acc: 0.0520\n",
      "Epoch 32/75\n",
      "211/211 [==============================] - 104s - loss: 0.1216 - acc: 0.0524 - val_loss: 0.1114 - val_acc: 0.0520\n",
      "Epoch 33/75\n",
      "211/211 [==============================] - 106s - loss: 0.1236 - acc: 0.0512 - val_loss: 0.1128 - val_acc: 0.0526\n",
      "Epoch 34/75\n",
      "211/211 [==============================] - 104s - loss: 0.1191 - acc: 0.0524 - val_loss: 0.1105 - val_acc: 0.0514\n",
      "Epoch 35/75\n",
      "211/211 [==============================] - 103s - loss: 0.1191 - acc: 0.0511 - val_loss: 0.1086 - val_acc: 0.0523\n",
      "Epoch 36/75\n",
      "211/211 [==============================] - 103s - loss: 0.1154 - acc: 0.0518 - val_loss: 0.1049 - val_acc: 0.0514\n",
      "Epoch 37/75\n",
      "211/211 [==============================] - 103s - loss: 0.1153 - acc: 0.0517 - val_loss: 0.1069 - val_acc: 0.0523\n",
      "Epoch 38/75\n",
      "211/211 [==============================] - 106s - loss: 0.1126 - acc: 0.0524 - val_loss: 0.1023 - val_acc: 0.0523\n",
      "Epoch 39/75\n",
      "211/211 [==============================] - 102s - loss: 0.1134 - acc: 0.0514 - val_loss: 0.1029 - val_acc: 0.0529\n",
      "Epoch 40/75\n",
      "211/211 [==============================] - 103s - loss: 0.1099 - acc: 0.0527 - val_loss: 0.0995 - val_acc: 0.0529\n",
      "Epoch 41/75\n",
      "211/211 [==============================] - 106s - loss: 0.1109 - acc: 0.0521 - val_loss: 0.1001 - val_acc: 0.0532\n",
      "Epoch 42/75\n",
      "211/211 [==============================] - 102s - loss: 0.1079 - acc: 0.0526 - val_loss: 0.0971 - val_acc: 0.0529\n",
      "Epoch 43/75\n",
      "211/211 [==============================] - 100s - loss: 0.1085 - acc: 0.0515 - val_loss: 0.0993 - val_acc: 0.0526\n",
      "Epoch 44/75\n",
      "211/211 [==============================] - 105s - loss: 0.1050 - acc: 0.0524 - val_loss: 0.0963 - val_acc: 0.0526\n",
      "Epoch 45/75\n",
      "211/211 [==============================] - 107s - loss: 0.1078 - acc: 0.0517 - val_loss: 0.0968 - val_acc: 0.0520\n",
      "Epoch 46/75\n",
      "211/211 [==============================] - 108s - loss: 0.1039 - acc: 0.0524 - val_loss: 0.0941 - val_acc: 0.0526\n",
      "Epoch 47/75\n",
      "211/211 [==============================] - 108s - loss: 0.1049 - acc: 0.0521 - val_loss: 0.0949 - val_acc: 0.0529\n",
      "Epoch 48/75\n",
      " 63/211 [=======>......................] - ETA: 63s - loss: 0.1024 - acc: 0.0561"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-000b85d6a651>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'models/checkpoints/model_{epoch:02d}.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nDone Training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, initial_epoch)\u001b[0m\n\u001b[1;32m   1115\u001b[0m                                         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, initial_epoch)\u001b[0m\n\u001b[1;32m   1807\u001b[0m                 \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1808\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1809\u001b[0;31m                     \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1811\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/carnd/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    634\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Train and save the model\n",
    "\n",
    "MODEL_DIR = \"models/\"\n",
    "\n",
    "source = 'track1'\n",
    "\n",
    "epochs = 75\n",
    "batch_size = 64\n",
    "\n",
    "img_ratio = 1   # = generator output per input image\n",
    "\n",
    "train_steps = (img_ratio * len(X_train)) // batch_size\n",
    "val_steps = len(X_valid) // batch_size\n",
    "\n",
    "train_gen = generator(X_train, y_train, source=source, val=False)\n",
    "val_gen = generator(X_valid, y_valid, source=source, val=True)\n",
    "\n",
    "checkpoint = ModelCheckpoint('models/checkpoints/model_{epoch:02d}.h5')\n",
    "    \n",
    "model.fit_generator(train_gen, steps_per_epoch=train_steps, epochs=epochs, \\\n",
    "                    validation_data=val_gen, validation_steps=val_steps, verbose=1, callbacks=[checkpoint])\n",
    "\n",
    "print('\\nDone Training')\n",
    "\n",
    "# Save model and weights\n",
    "model_json = model.to_json()\n",
    "with open(\"models/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save(\"models/model.h5\")\n",
    "print(\"Model saved to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit_generator(train_gen, steps_per_epoch=train_steps, epochs=epochs, \\\n",
    "                    validation_data=val_gen, validation_steps=val_steps, verbose=1, callbacks=[checkpoint])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit_generator(train_gen, steps_per_epoch=train_steps, epochs=epochs, \\\n",
    "                    validation_data=val_gen, validation_steps=val_steps, verbose=1, callbacks=[checkpoint])\n",
    "\n",
    "print('\\nDone Training')\n",
    "\n",
    "# Save model and weights\n",
    "model_json = model.to_json()\n",
    "with open(\"models/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save(\"models/model.h5\")\n",
    "print(\"Model saved to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Train and save the model\n",
    "\n",
    "MODEL_DIR = \"models/\"\n",
    "\n",
    "source = 'track1'\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 32\n",
    "\n",
    "img_ratio = 4   # max is 4 based on generator output per input image\n",
    "\n",
    "train_steps = (img_ratio * len(X_train_self)) // batch_size\n",
    "val_steps = len(X_valid_self) // batch_size\n",
    "\n",
    "train_gen = generator(X_train_self, y_train_self, source=source, val=False)\n",
    "val_gen = generator(X_valid_self, y_valid_self, source=source, val=True)\n",
    "\n",
    "checkpoint = ModelCheckpoint('models/checkpoints/model_{epoch:02d}.h5')\n",
    "    \n",
    "model.fit_generator(train_gen, steps_per_epoch=train_steps, epochs=epochs, \\\n",
    "                    validation_data=val_gen, validation_steps=val_steps, verbose=1, callbacks=[checkpoint])\n",
    "\n",
    "print('\\nDone Training')\n",
    "\n",
    "# Save model and weights\n",
    "model_json = model.to_json()\n",
    "with open(\"models/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save(\"models/model.h5\")\n",
    "print(\"Model saved to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
